% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learn_graph.R
\name{learn_graph}
\alias{learn_graph}
\title{Learn principal graph from the reduced dimension space using reversed graph
embedding}
\usage{
learn_graph(
  cds,
  use_partition = TRUE,
  close_loop = TRUE,
  learn_graph_control = NULL,
  verbose = FALSE
)
}
\arguments{
\item{cds}{the cell_data_set upon which to perform this operation}

\item{use_partition}{logical parameter that determines whether to use
partitions calculated during \code{cluster_cells} and therefore to learn
disjoint graph in each partition. When \code{use_partition = FALSE}, a
single graph is learned across all partitions. Default is TRUE.}

\item{close_loop}{logical parameter that determines whether or not to
perform an additional run of loop closing after estimating the principal
graphs to identify potential loop structure in the data space. Default is
TRUE.}

\item{learn_graph_control}{NULL or a list of control parameters to be
passed to the reversed graph embedding function. Default is NULL. A list
of potential control parameters is provided in details.}

\item{verbose}{Whether to emit verbose output during graph learning.}
}
\value{
an updated cell_data_set object
}
\description{
Monocle3 aims to learn how cells transition through a
biological program of gene expression changes in an experiment. Each cell
can be viewed as a point in a high-dimensional space, where each dimension
describes the expression of a different gene. Identifying the program of
gene expression changes is equivalent to learning a \emph{trajectory} that
the cells follow through this space. However, the more dimensions there are
in the analysis, the harder the trajectory is to learn. Fortunately, many
genes typically co-vary with one another, and so the dimensionality of the
data can be reduced with a wide variety of different algorithms. Monocle3
provides two different algorithms for dimensionality reduction via
\code{reduce_dimension} (UMAP and tSNE). Both take a cell_data_set object
and a number of dimensions allowed for the reduced space. You can also
provide a model formula indicating some variables (e.g. batch ID or other
technical factors) to "subtract" from the data so it doesn't contribute to
the trajectory. The function \code{learn_graph} is the fourth step in the
trajectory building process after \code{preprocess_cds},
\code{reduce_dimension}, and \code{cluster_cells}. After
\code{learn_graph}, \code{order_cells} is typically called.
}
\section{Optional \code{learn_graph_control} parameters}{

\describe{
\item{euclidean_distance_ratio:}{The maximal ratio between the euclidean
distance of two tip nodes in the spanning tree and the maximum distance
between any connecting points on the spanning tree allowed to be connected
during the loop closure procedure. Default is 1.}
\item{geodesic_distance_ratio:}{The minimal ratio between the geodesic
distance of two tip nodes in the spanning tree and the length of the
diameter path on the spanning tree allowed to be connected during the loop
closure procedure. (Both euclidean_distance_ratio and
geodesic_distance_ratio need to be satisfied to introduce the edge for
loop closure). Default is 1/3.}
\item{minimal_branch_len:}{The minimal length of the diameter path for a
branch to be preserved during graph pruning procedure. Default is 10.}
\item{orthogonal_proj_tip:}{ Whether to perform orthogonal projection for
cells corresponding to the tip principal points. Default is FALSE.}
\item{prune_graph:}{Whether or not to perform an additional round of graph
pruning to remove small insignificant branches. Default is TRUE.}
\item{scale:}{}
\item{ncenter:}{}
\item{rann.k:}{Maximum number of nearest neighbors to compute in the
reversed graph embedding. Set rann.k=NULL
to let learn_graph estimate rann.k. Default is 25.}
\item{maxiter:}{}
\item{eps:}{}
\item{L1.gamma:}{}
\item{L1.sigma:}{}
}
}

